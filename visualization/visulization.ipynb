{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2019 12:36:01 - INFO - multimodal_bert.multi_modal_bert -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "from multimodal_bert.datasets import ConceptCapLoaderTrain, ConceptCapLoaderVal\n",
    "from multimodal_bert.multi_modal_bert import BertForMultiModalPreTraining, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(validation_file=\"data/conceptual_caption/validation\",\n",
    "                       pretrained_weight= \"save/3layer_4connection/pytorch_model_6.bin\",\n",
    "                       bert_model=\"bert-base-uncased\",\n",
    "                       output_dir='save',\n",
    "                       config_file=\"config/3layer_4connection.json\",\n",
    "                       max_seq_length=36,\n",
    "                       train_batch_size=1,\n",
    "                       do_lower_case=True,\n",
    "                       predict_feature=False,\n",
    "                       seed=42,\n",
    "                       num_workers=0,\n",
    "                       from_pretrained=True,\n",
    "                       baseline=False,\n",
    "                       img_weight=1,\n",
    "                      )\n",
    "\n",
    "if args.baseline:\n",
    "    from pytorch_pretrained_bert.modeling import BertConfig\n",
    "    from multimodal_bert.bert import BertForMultiModalPreTraining\n",
    "else:\n",
    "    from multimodal_bert.multi_modal_bert import BertForMultiModalPreTraining, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2019 12:36:01 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/jiasen/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from data/conceptual_caption/validation_feat_all.lmdb\n",
      "\u001b[32m[0523 12:36:01 @format.py:92]\u001b[0m Found 6130 entries in data/conceptual_caption/validation_feat_all.lmdb\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_json_file(args.config_file)\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataset = ConceptCapLoaderVal(\n",
    "    args.validation_file,\n",
    "    tokenizer,\n",
    "    seq_len=args.max_seq_length,\n",
    "    batch_size=args.train_batch_size,\n",
    "    predict_feature=args.predict_feature,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "if args.predict_feature:\n",
    "    config.v_target_size = 2048\n",
    "    config.predict_feature = True\n",
    "else:\n",
    "    config.v_target_size = 1601\n",
    "    config.predict_feature = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2019 12:36:02 - INFO - multimodal_bert.multi_modal_bert -   loading archive file save/3layer_4connection/pytorch_model_6.bin\n",
      "05/23/2019 12:36:02 - INFO - multimodal_bert.multi_modal_bert -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bi_attention_type\": 1,\n",
      "  \"bi_hidden_size\": 1024,\n",
      "  \"bi_intermediate_size\": 3072,\n",
      "  \"bi_num_attention_heads\": 16,\n",
      "  \"fast_mode\": false,\n",
      "  \"fixed_t_layer\": 0,\n",
      "  \"fixed_v_layer\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"in_batch_pairs\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"predict_feature\": false,\n",
      "  \"t_biattention_id\": [\n",
      "    9,\n",
      "    10,\n",
      "    11,\n",
      "    12\n",
      "  ],\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"v_attention_probs_dropout_prob\": 0.1,\n",
      "  \"v_biattention_id\": [\n",
      "    0,\n",
      "    1,\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"v_feature_size\": 2048,\n",
      "  \"v_hidden_act\": \"gelu\",\n",
      "  \"v_hidden_dropout_prob\": 0.2,\n",
      "  \"v_hidden_size\": 2048,\n",
      "  \"v_initializer_range\": 0.02,\n",
      "  \"v_intermediate_size\": 3072,\n",
      "  \"v_num_attention_heads\": 16,\n",
      "  \"v_num_hidden_layers\": 3,\n",
      "  \"v_target_size\": 1601,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's option for predict_feature is  False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-beaeec88de68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMultiModalPreTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMultiModalPreTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/multi-modal-bert/multimodal_bert/multi_modal_bert.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, state_dict, cache_dir, from_tf, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m             state_dict = torch.load(\n\u001b[1;32m   1158\u001b[0m                 \u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             )\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtempdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 505\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "if args.from_pretrained:\n",
    "    model = BertForMultiModalPreTraining.from_pretrained(args.pretrained_weight, config)\n",
    "else:\n",
    "    model = BertForMultiModalPreTraining(config)\n",
    "    \n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "caption_path = \"data/conceptual_caption/caption_val.json\"\n",
    "captions = json.load(open(caption_path, 'r'))\n",
    "\n",
    "for step, batch in enumerate(validation_dataset):\n",
    "    image_id = batch[-1]\n",
    "    batch = tuple(t.cuda() for t in list(batch)[:-1])\n",
    "    \n",
    "    input_ids, input_mask, segment_ids, lm_label_ids, is_next, image_feat, image_loc, image_target, image_label, image_mask = (\n",
    "        batch\n",
    "    )\n",
    "\n",
    "    prediction_scores_t, prediction_scores_v, seq_relationship_score, all_attention_mask = model(\n",
    "        input_ids,\n",
    "        image_feat,\n",
    "        image_loc,\n",
    "        segment_ids,\n",
    "        input_mask,\n",
    "        image_mask,\n",
    "        output_all_attention_masks=True\n",
    "        )\n",
    "    all_attention_mask_t, all_attnetion_mask_v, all_attention_mask_c = all_attention_mask\n",
    "    \n",
    "    # visualization of the attention mask\n",
    "\n",
    "    idx = 0\n",
    "    image_id = image_id[0]\n",
    "    print(image_id)\n",
    "\n",
    "    image_path = 'data/conceptual_caption/validation/%s' %image_id\n",
    "\n",
    "    img = PIL.Image.open(image_path).convert('RGB')\n",
    "    img = torch.tensor(np.array(img))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    width, height,_ = img.shape\n",
    "    num_box = image_mask.sum().item()\n",
    "\n",
    "    image_loc = image_loc.squeeze(0)[:num_box,:4]\n",
    "\n",
    "#     print(input_ids.cpu().numpy())\n",
    "#     print(captions[image_id])\n",
    "    print(tokenizer.convert_ids_to_tokens(input_ids.cpu().numpy()[0]))\n",
    "    \n",
    "    words = tokenizer.convert_ids_to_tokens(input_ids.cpu().numpy()[0])\n",
    "    \n",
    "    attention_mask = all_attention_mask_c[0][0][0]\n",
    "    print(attention_mask.shape)\n",
    "    \n",
    "#     for i in range(16):\n",
    "#         aimg = bottomup_heatmap_image(img,attention_mask[i][7], image_loc)\n",
    "#         plt.imshow(aimg)\n",
    "#         plt.show()\n",
    "    print(step)\n",
    "    if step == 2 :\n",
    "        break\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bottomup_heatmap_image(img, att, spatial, cm=plt.get_cmap('viridis')):\n",
    "#     global hi1\n",
    "#     # color the background according to the colormap because everything's too small to see\n",
    "#     img[:] = 255.\n",
    "#     H, W, _ = img.shape\n",
    "#     n_head, _ = att.shape\n",
    "#     n_obj = spatial.shape[0]\n",
    "#     att_img = torch.zeros(img.shape)\n",
    "#     left =   (spatial[:, 0] * W).to(torch.int).clamp(0, W-1)\n",
    "#     top =    (spatial[:, 1] * H).to(torch.int).clamp(0, H-1)\n",
    "#     right =  (spatial[:, 2] * W).to(torch.int).clamp(0, W-1)\n",
    "#     bottom = (spatial[:, 3] * H).to(torch.int).clamp(0, H-1)\n",
    "# #     for i in range(n_head):\n",
    "#     if True:\n",
    "#     i = 1\n",
    "#         for k in range(n_obj):\n",
    "#             t, b, l, r = top[k], bottom[k], left[k], right[k]\n",
    "#             # this version is just black and white without using a color map\n",
    "#             att_img[t:b, l:r] += att[i,k] * img[t:b, l:r].to(att.dtype)\n",
    "#     att_img = att_img[:, :, 0].detach().numpy() / 255\n",
    "#     att_img = 255 * torch.tensor(cm(att_img))[:, :, :3]\n",
    "#     return att_img.to(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottomup_heatmap_image(img, att, spatial, cm=plt.get_cmap('viridis')):\n",
    "    global hi1\n",
    "    # color the background according to the colormap because everything's too small to see\n",
    "    img[:] = 255.\n",
    "    H, W, _ = img.shape\n",
    "    n_obj, _ = spatial.shape\n",
    "    att_img = torch.zeros(img.shape)\n",
    "    left =   (spatial[:, 0] * W).to(torch.int).clamp(0, W-1)\n",
    "    top =    (spatial[:, 1] * H).to(torch.int).clamp(0, H-1)\n",
    "    right =  (spatial[:, 2] * W).to(torch.int).clamp(0, W-1)\n",
    "    bottom = (spatial[:, 3] * H).to(torch.int).clamp(0, H-1)\n",
    "    for k in range(n_obj):\n",
    "        t, b, l, r = top[k], bottom[k], left[k], right[k]\n",
    "        # this version is just black and white without using a color map\n",
    "        att_img[t:b, l:r] += att[k] * img[t:b, l:r].to(att.dtype)\n",
    "    att_img = att_img[:, :, 0].detach().numpy() / 255\n",
    "    att_img = 255 * torch.tensor(cm(att_img))[:, :, :3]\n",
    "    return att_img.to(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_attention_mask_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5cd9ecb14bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# directly print the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_attention_mask_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mall_attention_mask_c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     for i in range(input_mask.sum()):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_attention_mask_c' is not defined"
     ]
    }
   ],
   "source": [
    "# directly print the weight\n",
    "\n",
    "for j in range(len(all_attention_mask_c)):\n",
    "    attention_mask =  all_attention_mask_c[j][0][0]\n",
    "#     for i in range(input_mask.sum()):\n",
    "    if True:\n",
    "        i = 10\n",
    "        print(words[i])\n",
    "        attention = attention_mask[:,i,:]\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "        # c = ax.pcolor(Z)\n",
    "        ax1.imshow(attention.cpu().detach().numpy())\n",
    "        ax1.axis('off')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        max_over_head, _ = torch.max(attention, dim=1)\n",
    "        _, idx = torch.max(max_over_head, dim=0)\n",
    "\n",
    "\n",
    "        aimg = bottomup_heatmap_image(img, attention[idx], image_loc)\n",
    "        ax2.imshow(aimg)\n",
    "        ax2.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img, spatial):\n",
    "    patches = []\n",
    "    H, W, _ = img.shape\n",
    "    left =   (spatial[:, 0] * W).to(torch.int).clamp(0, W-1)\n",
    "    top =    (spatial[:, 1] * H).to(torch.int).clamp(0, H-1)\n",
    "    right =  (spatial[:, 2] * W).to(torch.int).clamp(0, W-1)\n",
    "    bottom = (spatial[:, 3] * H).to(torch.int).clamp(0, H-1)\n",
    "    for i in range(spatial.size(0)):\n",
    "        patches.append(img[left[i]:right[i], top[i]:bottom[i]])\n",
    "    \n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the image based on the bounding box\n",
    "\n",
    "# print(image_loc)\n",
    "# patches = crop(img, image_loc)\n",
    "\n",
    "# num = len(patches)\n",
    "# for i, patch in enumerate(patches):\n",
    "#     f = plt.figure()\n",
    "#     ax1 = f.add_subplot(1,num, i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(patches[i])\n",
    "#     ax2 = f.add_subplot(1,num, i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(patches[i])\n",
    "#     # asp = np.diff(ax2.get_xlim())[0]/np.diff(ax2.get_ylim())[0]\n",
    "#     # ax2.set_aspect(asp)\n",
    "\n",
    "# #print(image_loc.shape[0])\n",
    "# # figs, axses = plt.subplots(1, image_loc.shape[0])#(1, image_loc.size(0), sharex='col', sharey='row',\n",
    "# #                         #gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "# # for i, axs in enumerate(axses):\n",
    "# #     axs.imshow(patches[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
