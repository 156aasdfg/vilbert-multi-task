{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import torch\n",
    "\n",
    "\n",
    "from bertviz.pytorch_transformers_attn import BertModel, BertTokenizer\n",
    "from bertviz.head_view_vilbert import show\n",
    "from vilbert.datasets import ConceptCapLoaderTrain, ConceptCapLoaderVal\n",
    "from bertviz.pytorch_transformers_attn.vilbert import VILBertForVLTasks, BertConfig, BertForMultiModalPreTraining\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2019 21:58:37 - INFO - bertviz.pytorch_transformers_attn.vilbert -   loading archive file /coc/pskynet2/jlu347/multi-modal-bert/save/bert_base_6_layer_6_connect_freeze_0/pytorch_model_8.bin\n",
      "07/28/2019 21:58:37 - INFO - bertviz.pytorch_transformers_attn.vilbert -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bi_attention_type\": 1,\n",
      "  \"bi_hidden_size\": 1024,\n",
      "  \"bi_intermediate_size\": 1024,\n",
      "  \"bi_num_attention_heads\": 8,\n",
      "  \"fast_mode\": false,\n",
      "  \"fixed_t_layer\": 0,\n",
      "  \"fixed_v_layer\": 0,\n",
      "  \"fusion_method\": \"mul\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"in_batch_pairs\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"intra_gate\": false,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooling_method\": \"mul\",\n",
      "  \"predict_feature\": false,\n",
      "  \"t_biattention_id\": [\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    11\n",
      "  ],\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"v_attention_probs_dropout_prob\": 0.1,\n",
      "  \"v_biattention_id\": [\n",
      "    0,\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4,\n",
      "    5\n",
      "  ],\n",
      "  \"v_feature_size\": 2048,\n",
      "  \"v_hidden_act\": \"gelu\",\n",
      "  \"v_hidden_dropout_prob\": 0.1,\n",
      "  \"v_hidden_size\": 1024,\n",
      "  \"v_initializer_range\": 0.02,\n",
      "  \"v_intermediate_size\": 1024,\n",
      "  \"v_num_attention_heads\": 8,\n",
      "  \"v_num_hidden_layers\": 6,\n",
      "  \"v_target_size\": 1601,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"with_coattention\": true\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's option for predict_feature is  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2019 21:58:44 - INFO - bertviz.pytorch_transformers_attn.vilbert -   Weights from pretrained model not used in BertForMultiModalPreTraining: ['bert.encoder.c_layer.0.biattention.scale.weight', 'bert.encoder.c_layer.1.biattention.scale.weight', 'bert.encoder.c_layer.2.biattention.scale.weight', 'bert.encoder.c_layer.3.biattention.scale.weight', 'bert.encoder.c_layer.4.biattention.scale.weight', 'bert.encoder.c_layer.5.biattention.scale.weight']\n",
      "07/28/2019 21:58:50 - INFO - bertviz.pytorch_transformers_attn.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /nethome/jlu347/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from /coc/pskynet2/jlu347/multi-modal-bert/data/conceptual_caption/validation_feat_all.lmdb\n",
      "\u001b[32m[0728 21:58:50 @format.py:92]\u001b[0m Found 6130 entries in /coc/pskynet2/jlu347/multi-modal-bert/data/conceptual_caption/validation_feat_all.lmdb\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "from types import SimpleNamespace\n",
    "\n",
    "root_path = '/coc/pskynet2/jlu347/multi-modal-bert/'\n",
    "\n",
    "args = SimpleNamespace(validation_file=root_path + \"data/conceptual_caption/validation\",\n",
    "                       from_pretrained= root_path + \"save/bert_base_6_layer_6_connect_freeze_0/pytorch_model_8.bin\",\n",
    "                       bert_model=\"bert-base-uncased\",\n",
    "                       output_dir=root_path + 'save',\n",
    "                       config_file=root_path + \"config/bert_base_6layer_6conect.json\",\n",
    "                       max_seq_length=36,\n",
    "                       train_batch_size=1,\n",
    "                       do_lower_case=True,\n",
    "                       predict_feature=False,\n",
    "                       seed=42,\n",
    "                       num_workers=0,\n",
    "                       baseline=False,\n",
    "                       img_weight=1,\n",
    "                       distributed=False,\n",
    "                      )\n",
    "config = BertConfig.from_json_file(args.config_file)\n",
    "if args.predict_feature:\n",
    "    config.v_target_size = 2048\n",
    "    config.predict_feature = True\n",
    "else:\n",
    "    config.v_target_size = 1601\n",
    "    config.predict_feature = False\n",
    "\n",
    "if args.from_pretrained:\n",
    "    model = BertForMultiModalPreTraining.from_pretrained(args.from_pretrained, config)\n",
    "else:\n",
    "    model = BertForMultiModalPreTraining(config)\n",
    "    \n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda: model = model.cuda(0)\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "\n",
    "validation_dataset = ConceptCapLoaderVal(\n",
    "    args.validation_file,\n",
    "    tokenizer,\n",
    "    seq_len=args.max_seq_length,\n",
    "    batch_size=args.train_batch_size,\n",
    "    predict_feature=args.predict_feature,\n",
    "    num_workers=0,\n",
    "    distributed=args.distributed,\n",
    "    visualization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a batch\n",
    "import random\n",
    "burn_in = int(random.random() * 5000)\n",
    "print(burn_in)\n",
    "for step, batch in enumerate(validation_dataset):\n",
    "    image_id = batch[-1][0]\n",
    "\n",
    "    batch = tuple(t.cuda() for t in list(batch)[:-1])\n",
    "    input_ids, input_mask, segment_ids, lm_label_ids, is_next, image_feat, image_loc, image_target, image_label, image_mask = (\n",
    "        batch\n",
    "    )\n",
    "    \n",
    "    if step > burn_in and is_next[0] == 0:\n",
    "        break\n",
    "\n",
    "# image_id = int(image_ids[0].item())\n",
    "# show the image.\n",
    "\n",
    "image_path = '/srv/datasets/conceptual_caption/validation/%s' %image_id\n",
    "img = PIL.Image.open(image_path).convert('RGB')\n",
    "img = torch.tensor(np.array(img))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "# show the caption.\n",
    "\n",
    "sents = input_ids.cpu().numpy().tolist()[0]\n",
    "sents = tokenizer.convert_ids_to_tokens(sents)\n",
    "sents = [sent for sent in sents if sent != \"[PAD]\"]\n",
    "print(\" \".join(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the patch for each image index\n",
    "num = image_mask.sum().item()\n",
    "image_label_list = image_label[0].tolist()\n",
    "image_label_list.insert(0,-1)\n",
    "height, width, _ = img.shape\n",
    "print(\"image height: %d, width %d\" %(height, width))\n",
    "\n",
    "examples_per_row = 5\n",
    "nrows, ncols = int(num / examples_per_row)+1, examples_per_row \n",
    "figsize = [ncols*3, ncols*5]     # figure size, inches\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    if i < num:\n",
    "        box = image_loc[0][i][:4].tolist()\n",
    "        y1 = int(box[1] * height)\n",
    "        y2 = int(box[3] * height)\n",
    "        x1 = int(box[0] * width)\n",
    "        x2 = int(box[2] * width)\n",
    "#         print([y1,x1,y2,x2])\n",
    "        patch = img[y1:y2,x1:x2]\n",
    "        axi.imshow(patch)\n",
    "        axi.axis('off')\n",
    "        if image_label_list[i] == 1:\n",
    "            axi.set_title(str(i) + '(masked)')\n",
    "        else:\n",
    "            axi.set_title(str(i))\n",
    "            \n",
    "plt.axis('off')\n",
    "plt.tight_layout(True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bertviz.model_view_vilbert import show\n",
    "show(model, tokenizer, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
